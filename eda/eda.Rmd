---
title: "Exploratory Data Analysis: EDA"
output: 
  html_notebook: 
    fig_width: 13
    number_sections: yes
    toc: yes
    toc_depth: 2
---




```{r, message=FALSE, warning=FALSE, include=FALSE}
rm(list = ls())

instalar <- function(paquete) {

    if (!require(paquete,character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)) {
        install.packages(as.character(paquete), dependecies = TRUE, repos = "http://cran.us.r-project.org")
        library(paquete, character.only = TRUE, quietly = TRUE, warn.conflicts = FALSE)
    }
}

paquetes <- c('lubridate', 'magrittr', 'ggvis', 'dplyr', 'tidyr', 'readr', 'rvest', 
              'ggplot2', 'stringr', 'ggthemes', 'googleVis', 'shiny', 'tibble', 'vcd', 'vcdExtra',
              'GGally', 'readODS', 'readxl', "RSQLite")

lapply(paquetes, instalar);

source("metadata.R")
source("utils.R")
```

# GDA

## Exploratory Data Analysis

- Es importante conocer tus datos **siempre**
- Es principalmente usada para ayudarte con el modelado
- Pero es clave para la limpieza de los datos, tranformación, etc.
- También será  importante para la automatización de los modelos 
- Servirá para detectar si exite algún *drift* de los datos.
- Una parte muy importante es hacer esto **visual** (por lo menos las primeras veces): **GEDA**.
    - Las secciones siguientes (hasta antes de **Casos de estudio**) están basadas en el libro *Graphical Data Analysis with R*.

## Algunos consejos

- No existe un tipo óptimo de gráfica, prueba varias.
- Siempre debes de pensar que esperas ver, antes de dibujar la gráfica


## Tipos de variables

- Numéricas continuas

- No numéricas
    - Categóricas nominales
    - Categóricas ordinales
    - Datos discretos
        - Conteos y Enteros

- *Texto*, *Imágenes*, *Audio*, etc.
    - Ver *feature engineering* más adelante en el curso

## Gráficas

### Variables contínuas

- ¿Qué podemos encontrar?
    - Asimetría
    - *Outliers*
    - Multimodalidad
    - Huecos (*Gaps*)
    - *Heaping*
    - Redondeos
    - Imposibilidades
    - Errores

- ¿Cómo visualizarlas?
    - *Histogramas* Muestran la distribución empírica (*raw*)
        - ¡*Empírica*!  No son estimadores...
    - *Boxplot* Despliega *outliers*. Estadística Robusta (tener cuidado con el paquete que se utilice). Comparación entre subgrupos.
    - *Dotplot* Cada observación un punto, permnite detectar *huecos* en los datos
    - *Rugplot* Cada observación como una pequeña línea en el eje horizontal, se usa en conjunto con otras gráficas
    - *Density estimate* Es como un *modelo* de los datos
         - Sólo recuerda los límites de las variables, sobre si todo si son estrictos: *siempre* revisa la documentación.
    - *Distribution estimate* Útil para comparar distribuciones (e.g. ver si una está adelante de la otra)
    - *QQ plot* Compara la distribución contra una distribución teórica (por *default* es la distribución normal)

#### Ejemplos    

El *dataset* `Boston` del paquete **MASS** contiene 14 variables de 506 áreas alrededor de la ciudad de Boston.

```{r}
glimpse(MASS::Boston)
```

Una de sus principales variables es el valor medio de las casas por área, la varoable `medv`. Podemos 
utilizar visualizarlo con una tabla, pero salvo el hecho de que todo está redondeado a un decimal, no 
es fácil sacar más información.

```{r}
with(MASS::Boston, table(medv))
```


    
```{r}
with(MASS::Boston, hist(medv))
```
    

Hay dos cosas que notar en esta gráfica, la caída alreddor de 25 y lo inusual que 
está el valor en 50. ¿Quizá sea por el tamaño del *bin*?
    
```{r}
ggplot(MASS::Boston, aes(medv)) + 
    geom_histogram() +
    ylab("") + xlab("Valor medio de las casas (1000s USD)")
```

```{r}
ggplot(MASS::Boston, aes(medv)) + 
    geom_histogram(binwidth=1) +
    ylab("") + xlab("Valor medio de las casas (1000s USD)")
```

¿Qué pasa con las demás variables?

```{r fig.width=13}
B2 <- gather(MASS::Boston, BosVars, BosValues, crim:medv)
ggplot(B2, aes(BosValues)) +
    geom_histogram() + xlab("") + ylab("") +
    facet_wrap(~ BosVars , scales = "free")
```
Esta gráfica sirve como punto inicial, pero el tamaño de los *bines* no beneficia a todas las variables 
por igual, además la escala varía muchísimo entre las variables (entre 40 y 400). Por último
el `anchorpoint` (i.e. donde empieza el primer *bin* y si está abierto por la izquiera o la derecha)
puede ser importante, como muestra el siguiente ejemplo:

```{r}
with(MASS::Boston, hist(ptratio, freq = FALSE))
```

```{r}
with(MASS::Boston, MASS::truehist(ptratio))
```

**Ejercicio**

1. Grafica `medv` usando `boxplot`, `stripchart` (*jittered dotplot*), `stem`, `density` estimate con  `rug` plot

```{r}
?boxplot
?stripchart
?stem
?density
?rug
```

¿Qué observas en cada una de estas gráficas?

2. En la gráfica de las 14 variables mostrada arriba ¿Cómo describirías las distribuciones? 
  ¿Para cuales variables sería mejor utilizar `boxplot`? ¿Por qué?


Los *outliers* son difíciles de tratar de manera automática y son muy importantes de resolver

```{r}
glimpse(ggplot2movies::movies)
```


```{r}
ggplot(ggplot2movies::movies, aes(length)) + geom_histogram() + ylab("")
```

El histograma no resultó de mucha utilidad en esta ocasión.

```{r}
ggplot2movies::movies %>% 
    filter(length > 2000) %>%
    select(length, title) %>%
    arrange(desc(length))
```

```{r}
ggplot(ggplot2movies::movies, aes("var", length)) + 
    geom_boxplot() + xlab("") + scale_x_discrete((breaks = NULL)) + coord_flip()
```

Podemos ver el histograma si recortamos esas variables

```{r}
ggplot(ggplot2movies::movies, aes(x=length)) + 
    xlim(0, 180) +
    geom_histogram(binwidth = 1) +
    xlab("Duración de películas en minutos") + ylab("") 
```
**Ejercicio** 

1. ¿Qué puedes decir de esta gráfica?
2. ¿Cómo la modificas para agregar más *ticks*?
3. Haz una gráfica que muestre que los picos a los 7 y 90 minutos existían antes y después de 1980
4. Existe la variable `Short` que indica si la película es "corta" ¿Qué gráfica puedes  hacer para
   ver que criterio se utilizó para definir esta variable y cuáles están mal clasificadas?
    
### Variables categóricas

- ¿Qué podemos encontrar?
    - Patrones insesperados en los resultados
    - Malas Distribuciones 
    - Categorías extras
    - Experimentos no balanceados
    - Muchas categorías
    - "No sé", Errores, Faltantes
    
- ¿Cómo visualizarlas?
    - *Barchars*: Nominales, Ordinales, Variables discretas
    - Explora (o ten cuidado con) diferentes ordenamientos

### Dos variables contínuas

- ¿Qué podemos encontrar?
    - Relaciones lineales y no lineales
    - Asociaciones
    - *Outliers* 
        - Se puede ser *outlier* en una dimensión y no serlo en dos, o viceversa
    - *Clusters*
    - *Gaps*
    - Barreras
    - Relación condicional

- Modelos y pruebas
    - Correlación
    - Regresión lineal
    - *Smoothing*

#### Ejemplos

Podemos continuar con la base de datos `movies`.  En particular veamos las variables `rating`,
la cual representa el promedio de calificaciones en **IMDB** y `votes`, número de personas que
calificaron la película.

```{r}
ggplot(ggplot2movies::movies, aes(votes, rating)) + geom_point() + ylim(1,10)
```

**Ejercicio**

1. Agrega *alpha-blending* ¿Qué pasa con los  *outliers*? ¿Diferentes valores funcionan mejor?
1. ¿Cómo se ve la gráfica si  remueves las películas con menos de 100 votos?
2. ¿Cómo si remueves todas las películas que tienen un *rating* arriba de 9?

Al igual que en el caso univariado, es posible estudiar posibles modelos, como se muestra en el siguiente ejemplo,
usando el *dataset* `Cars93` del paquete **MASS**:

```{r}
ggplot(MASS::Cars93, aes(Weight, MPG.city)) + geom_point() +
    geom_smooth(colour="green") + ylim(0,50)
```

**Ejercicio**

- ¿Cuál es el *outlier* de la izquierda?

- En muchos países en lugar de medirse el desempeño en millas por galón, se mide  en 
  litros por 100 km. ¿Qué pasa si graficas `1/MPG.city` contra `Horsepower`? ¿Existe una relación 
  lineal?¿Cuáles son los outliers?
  
Al igual que cuando graficamos todas las variables del *dataset* `Boston`, podemos hacer un
**splom** (*Scatterplot matrix*)

```{r, fig.height=13, fig.width=13}
MASS::Boston %>%
    select(-rad,-chas) %>%
    ggpairs(title="Boston Dataset", diag=list(continuos='density', axisLabels='none'))
```

**Ejercicio**
- ¿Cuáles están positivamente correlacionadas con `medv`?
- La variable `crim` (tasa de crímenes per cápita) tiene *scatterplots* con forma inusual,
  donde los valores más altos de `crim` sólo ocurren para una valor de la otra variable
  ¿Qué explicación le puedes dar?
- Hay varias formas en los *scatterplots*, escoge 5 y explica como las interpretas?

## Más de dos variables contínuas

Básicamente lo que queda aquí es el *parallel coordinate plot* (**pcp**). Estos 
diagramas dan vistazos rápidos a las distribuciones univariadas de varias variables a la vez:
si son *skew*, si hay *outliers*, si hay *gaps*, etc.

El famoso `iris` *dataset* nos permite ver algunas cosas con este tipo de gráfica

```{r}
ggparcoord(iris, columns = 1:4, groupColumn = "Species")
```

Hay varias cosas a notar, primero, lo que una observa en la gráfica depende del orden 
en el cual se dibujan los ejes. Hay autores que sugieren intentar varias combinaciones o 
inclusive poner varias copias de los ejes. Quizá lo mejor sea tener una gráfica 
interactiva para tal efecto (ver más adelante en el curso).

Otro tema es el de *outliers* y escalamiento de las variables. Este tipo de gráficas
se puede usar para comparar modelos, series de tiempo, analizar *clusters*, índices, etc
Estos temas los discutiremos más adelante también.

Hay varias cosas que se pueden ajustar en esras gráficas, las más importantes quizá sean 
el orden de las variables y el *escalamiento*. En el caso del  escalamiento,
recuerda que cuando vas a comparar diferentes mediciones tienes que determinar alguna 
manera de ponerlas en la misma escala.

En las gráficas **pcp** por *default*, la escala depende del máximo y mínimo de cada variable
(si, para cada eje), el máximo se mapeará a $1$ y el mínimo a $0$.

$$
y_{ij} = \frac{x_{ij} - \min_i x_{ij}}{\max_i x_{ij} - \min_i x_{ij}}
$$

Otro posible escalamiento sería usar la desviación  estándar o el rango intercuartil (**IQR**), por ejemplo:

$$
z_{ij} = \frac{x_{ij} - \bar{x}_j}{sd(x_j)}
$$

La gráfica recién mostrada  utiliza este escalamiento. Este es el mapeo por omisión de `ggparcoord`. 


```{r,fig.height=13, fig.width=13}
iris1 <- iris
names(iris1) <- c(abbreviate(names(iris)[1:4]), "Species")
a1 <- ggparcoord(iris1, columns = 1:4, alphaLines = 0.7,  groupColumn = "Species") + xlab("") + ylab("") + ggtitle("a1")
a2 <- ggparcoord(iris1, columns = 1:4, scale="uniminmax", alphaLines=0.7, groupColumn = "Species") + xlab("") + ylab("") + ggtitle("a2")
a3 <- ggparcoord(iris1, columns = 1:4, scale="globalminmax", alphaLines=0.7, groupColumn = "Species") + xlab("") + ylab("") + ggtitle("a3")
a4 <- ggparcoord(iris1, columns = 1:4, scale="center", scaleSummary="median", alphaLines=0.7, groupColumn = "Species") + xlab("") + ylab("") + ggtitle("a4")

gridExtra::grid.arrange(a1, a2, a3, a4)
```

**Ejercicio**

- Usando el `Boston` *dataset* realice un `pcp`. Trate de resaltar las características que ha observado en los 
ejercicios anteriores. Piensa como le hiciste


### Varias variables categóricas

Aunque existen pocas opciones para una variable categórica (de hecho una, si eliminamos la gráfica de *pie*),
cuando tratamos con varias variables categóricas la situación se complica muchísimo, debido a la gran cantidad
de posibles ordenamientos de las variables. (Para $J$ variables categóricas con $c$ número de categorías, las variables)
pueden ser ordenadas de $J!$ maneras diferentes y las categorías dentro de las variables de $\Pi_{j=1}^J c_j!$, lo cual da
un total de $J!\Pi_{j=1}^Jc_j!$ ordenamientos.

En cuanto a los tipos de gráficos tenemos *mosaicplots*, *doubledecker plots*, *fluctuation diagrams*, *treemaps*, *association plots* 
y *parallel sets / categorical parallel coordinate plots*.


Parte de la discusión siguiente está basada en el artículo [*New approaches in Visualization of Categorical Data: `R` Package `extracat`* de A. Pilhöfer y A. Unwin. JSS, Vol53 issue 7, May 2013](https://www.jstatsoft.org/article/view/v053i07/v53i07.pdf).




La primera de estas gráficas que veremos es un `doubledecker`, en ella las variables se dividen en 
variables explicativas y variable objetivo. Esta última ocupa el eje vertical.

```{r fig.width=13}
doubledecker(Survived ~ Sex, data = Titanic, gp=gpar(fill=c("grey90", "red")))
```

```{r fig.width=13}
doubledecker(Survived ~ Class, data = Titanic, gp=gpar(fill=c("grey90", "red")))
```

Conforme agregamos más variables se vuelve más díficil de interpretar.

```{r fig.width=13}
doubledecker(Survived ~ Sex + Class, data = Titanic, gp=gpar(fill=c("grey90", "red")))
```

Además, como todas las gráficas de tipo *mosaic*, la construcción de la gráfica depende del orden de las variables:

```{r fig.width=13}
doubledecker(Survived ~ Class + Sex, data = Titanic, gp=gpar(fill=c("grey90", "red")))
```

En un `mosaicplot` la gráfica está dividida en rectángulos proporcionales ea los conteos de las
combinaciones que los rectangulos representan. Se dibujan partiendo por un rectángulo que representa
todo el *dataset* , luego se toma la primera variable y se parte el eje horizontal en secciones proporcionales
a los tamaños de las categorías.

```{r fig.width=13}
titanic <- as.data.frame(Titanic)
par(mfrow=c(2,2),  mar= c(4, 4, 0.1, 0.1))
mosaicplot(xtabs(Freq ~ Survived, data=titanic), main="")
mosaicplot(xtabs(Freq ~ Survived + Sex, data=titanic), main="")
mosaicplot(xtabs(Freq ~ Survived + Sex + Class, data=titanic), main="")
mosaicplot(xtabs(Freq ~ Survived + Sex + Class + Age, data=titanic), main="")
```
De nuevo el orden de las variables determinará la apariencia (y quizá el *insight* que puedes obtener)


Otra opción es utilizar la gráfica `pairs`

```{r fig.width= 13}
pairs(xtabs(Freq ~ ., data=titanic))
```

La diagonal de esta gráfica contiene gráficas de barra para las variables individuales y *mosaicplots* en los
elementos que no están en la diagonal. Si hay muchas variables esta gráfica será muy difícil de leer.

Si existe una variable a explicar obvia (en nuestro caso `Survived`), quizá sea mejor tomar esto en cuenta en nuestro análisis:

```{r fig.width=13}
ggplot(titanic, aes(Survived, Freq, fill=Sex)) + 
    geom_bar(stat = "identity") +
    facet_grid(Class ~ Sex + Age) + theme(legend.position="none")
```

Si queremos observar tablas de contingencia muy grandes o matrices de confusión, podemos usar las gráficas 
de tipo *fluctile*. En particular estas gráficas resaltan qué subgrupos aparecen más frecuentemente. O cuales 
combinaciones  no aparecen en lo absoluto.

```{r fig.width=13}
## Nota que estamos usando Titanic y no titanic!
extracat::fluctile(Titanic)
```

Por otro lado, si sólo nos interesa comparar las tasas, podemos usar la función `rmb` con el parámetro `freq.trans="const"`:

```{r fig.width=13}
extracat::rmb(formula = ~Sex+Class+Age+Survived, data=titanic, cat.ord=2, spine=TRUE, freq.trans="const")
```

En esta gráfica estamos viendo las tasas de supervivencia (el color verde) por subgrupo. Esto sería muy dificil de apreciar con un *mosaicplot* normal. 

El nombre de estas gráficas es  *relative multiple barcharts*, las cuales mezclan las gráficas de barras 
y los *mosaicplots*.  

# Forma *deseable*  de los datos

Para mucho de los análisis queremos que los datos estén en formato *tidy*.

- Ver el artículo [**Tidy Data**](http://vita.had.co.nz/papers/tidy-data.pdf) de Hadley Wickham

- Pero hay ejemplos donde no queremos *tidy data* e.g. [**Non Tidy Data**](http://simplystatistics.org/2016/02/17/non-tidy-data/)

- O grafos (ver más adelante)
    


![Tidy Data](http://r4ds.had.co.nz/images/tidy-1.png)

*Fuente: R for Data Science, Wickham and Grolemund, 2016*

Es decir:

1. Cada *variable* una columna
2. Cada *observación* un renglón
3. Cada *valor* una celda

### Proceso: ¿Dónde estamos?

![Limpieza y Transformación](../images/Data_Analysis_and_Cleaning.png)

*Fuente: Presentaciones de Hadley Wickham, por ejemplo [esta](http://vita.had.co.nz/papers/tidy-data-pres.pdf)*

### Formatos *on the wild*

#### Ejemplos de formatos de *datasets*  que se pueden obtener del INEGI (por lo menos al 2014)

| | Lat | Long | Indicador |
|--|----|------|-----------|
|Obs 1 | # | # | # |

Table: Fecha implícita 

| | lugar | indicador |
|---|-----|-----|-----|
|obs 1 |  | |
|obs 2 | |  |

Table: Otro con Fecha implícita 


| | |Fecha 1 | Fecha 2 |
|---|-----|------|----|
|lugar 1 |   |    |   |
| lugar 2 | |  | |

Table: Implícita la variable

|  |  Fecha 1 | Fecha 2 | ... |
|---|-------|-------|------|
| **LUGAR 1** |   |    |   |  |
| Ind 1 |   #  | #   | ... |
| Ind 2 |   #  |  #   | ... |
| **LUGAR 2** |   |    |   |  |
| Ind 1 |   #  | #   | ... |
| Ind 2 |   #  |  #   | ... |

Table: ¿¿Por qué??

Esto es un desastre...

| $\quad \qquad$ |   Indicador 1  |   Indicador 2 |
|-----|:--------------|:--------------|

|   | Fecha 1 | Fecha 2 | Fecha 1 | Fecha 2 |
|----|-------|--------|--------|--------|
|lugar 1 | Ind 1    |  Ind 1 | Ind 2     | Ind 2   |
|lugar 1 | Ind 1   |   Ind 1 | Ind 2     | Ind 2   |



| | | Ind 1 | Ind 2 |
|---|-----|------|----|
|lugar 1 |   |    |   |
| lugar 2 | |  | |

Table: Implícita la fecha (de nuevo)

En este formato, se muestran diferentes transformaciones de los indicadores en varias fechas.

| $\quad \qquad$ |  Transformación 1  |   Transformación 2 |
|-----|:--------------|:--------------|

|   | Fecha 1 | Fecha 2 | Fecha 1 | Fecha 2 |
|----|-------|--------|--------|--------|
| **LUGAR 1** |   |    |   |  |
| Ind 1 |   #  | #   | ... |
| Ind 2 |   #  |  #   | ... |
| **LUGAR 2** |   |    |   |  |
| Ind 1 |   #  | #   | ... |
| Ind 2 |   #  |  #   | ... |




|      |  Lugar |  Variable |  SubFecha 1 | SubFecha 2 | ..... |
|------|--------|-----------|-------------|-------------|-------|      
|Fecha 1 | lugar 1 | ind 1 |     |     |    |
|Fecha 1 | lugar 1 | ind 2 |     |     |    |
|Fecha 1 | lugar 2 | ind 1 |     |     |    |
|Fecha 2 | lugar 1 | ind 1 |     |     |    |
|Fecha 2 | lugar 1 | ind 1 |     |     |    |

Table: Fecha en esta tabla se refiere, por ejemplo a un año, sub fechas a los meses


|      |       | Variable |       |
|------|-------|----------|:-------:|
| Fecha 1 | Lugar 1 | ind 1  |    #   |
| Fecha 2 | Lugar 1 | ind 2  |    #   |
| Fecha 1 | Lugar 2 | ind 1  |    #   |
| Fecha 2 | Lugar 2 | ind 2  |    #   |

Table: Esto es lo más cercano a una tabla `tidy`

|      |  Fecha 1  |   Fecha 2   |   Fecha 3 | .... |
|------|-----------|-------------|-----------|------|
| Lugar 1 |        |             |           |      |
| Lugar 2 |        |             |           |      |
| Lugar 3 |         |             |           |      |

Table: Aquí, de nuevo, está implícita la variable.


|         | Lugar A | Lugar B | ..... |          
|---------|---------|---------|-------|
| Lugar 1 |   #    |     #    | #   | 
| Lugar 2 |   #    |     #    | #   | 
| Lugar 3 |   #    |     #    | #   | 

Table: Esta es como una matriz que puede indicar como se relacionan los lugares `A`, `B` $\ldots$ con
los lugares `1`, `2`, $\ldots$. Además este formato se puede complicar si se usan *subcolumnas* con fechas.
La variable está implícita.



Otros ejemplos:

- Nombres de las columnas representan valores de los datos en lugar de nombres de variables
- Una columna contiene varias variables en lugar de una variable
- Una tabla contiene más de una unidad de observación
- Las variables están contenidas en los renglones y columnas, en lugar de sólo columnas.
- Los datos de una unidad observacional están dispersas en varios *data sets*

# Casos de estudio

## Reproducibilidad

En estos casos de estudio nos vamos a encontrar con nuestro primer tipo de 
*pipeline*, en este caso en particular, este *pipeline* **no** es para ejecutar
grandes volúmenes de datos o para ejecutar contínuamente, si no para poder
reproducir el proceso de exploración y modelado de datos.

**Ejercicio**

Crea en tu carpeta las carpetas `german` y `algas`, dentro de ellas crea los archivos 
`00-load.R`, `01-prepare.R` y `02-clean.R` y `run.R`
En estos archivos pondrás, respectivamente el código para ejecutar los *pipelines* siguientes de los 
casos de estudio.


## `German`

### ¿Quién eres? 

Eres el científico de datos de un banco alemán, el banco tiene muchas pérdidas debido a malos créditos y quiere reducirlas. 
Te piden realizar esta tarea, indicando que quieren reducir la tasa de pérdidas en un 10%.

### Datos  

Usaremos para este ejemplo, los datos de crédito alemán (*German data set*). 
[German Credit Data](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29)



### Carga de datos 

```{r, message=FALSE, warning=FALSE}

german_url <- paste('http://archive.ics.uci.edu/ml',
                    '/machine-learning-databases/statlog',
                    '/german/german.data',
                    sep='')

german_data <- read_delim(german_url, 
                          col_names=FALSE, 
                          delim = " ")
```

Los datos son un asco...

```{r}
german_data
```

**Ejercicio**

- Crea una función `load` en `utils.R` en tu carpeta, que descargue, si y sólo si no existe
un archivo `german.rds`. Si no existe, descarga y guarda el archivo.

- `?saveRDS`, `?readRDS`

### Transformación de datos

Los nombres de las columnas fueron copiados a mano desde [`german.doc`](https://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.doc)

```{r}
german_colnames
```

La variable de salida la estoy definiendo como categórica (`factor` en `R`)
```{r}
colnames(german_data) <- german_colnames

german_data$good_loan <- as.factor(
                          ifelse(
                            german_data$good_loan == 1, 
                            'GoodLoan', 
                            'BadLoan'
                            )
                          )
```

### Decodificar

- Crea una función `german_decode` en un archivo `utils.R` dentro de tu carpeta, 
esta función debe de utilizar `german_codes` (en el archivo `metadata.R`) para 
decodificar los elementos  de todas las columnas (por ejemplo `A201` -> `yes`)

- Utiliza `dplyr` para decodificar todas las columnas de `german_data`

```{r, warning=FALSE}
german_data  <- german_data %>% 
                     mutate_each(funs(german_decode))

german_data
```

### Datos manejables 

En este momento deberás de tener archivos `00-load.R`, `01-prepare.R`, `02-clean.R`,  `metadata.R` 
y un archivo `utils.R` dentro de `german`.  
Además deberías de tener un archivo `german.rds`.

**Ejercicio**

- ¿Hay algo raro con los datos de préstamo?

- ¿Cuál crees que debería ser la distribución del resultado del 
  préstamo `Good_Loan` respecto a `Credit history`?

- Grafícalo y comenta tus resultados.

- Si lo vas a hacer con `ggplot2` usa este 
[cheatsheet](https://www.rstudio.com/wp-content/uploads/2015/12/ggplot2-cheatsheet-2.0.pdf) 
o estos [ejemplos](http://docs.ggplot2.org/current/)

**Ejercicio**

- Fue terrible poder hacer la gráfica con `ggplot2` utilizando los nombres de
columnas que pusimos (`german_colnames`).

- Modifica el archivo donde tengas `german_colnames` (puede ser `utils.R` o `metadata.R`) y 
sustituye (usando quizá `stringr` o `grep`) los `' '` y `'/'` por `'_'` (ve la [guía de 
estilo](http://adv-r.had.co.nz/Style.html)) y pasa todo a minúsculas.

- Ejecuta todo de nuevo (¡la ventaja de ser reproducible!)

```{r}
colnames(german_data) <- german_clean_colnames(german_colnames)
```


### Intermedio

- Si te quedó desacomodado, este código ordena los `bar charts`

```{r fig.width=13}
german_data %>% 
    group_by(credit_history) %>% 
    dplyr::summarise(count = n()) %>% 
    arrange(desc(count)) %>% 
    ggplot(.) + 
        geom_bar(aes(x=reorder(credit_history, count), y = count), stat="identity", fill="gray") + 
        coord_flip() + 
        theme_hc() + 
        ylab('casos') + 
        xlab('Historial de crédito')
```

### Sanidad de los datos

- El nombre de la columna no significa lo que tu crees que significa

- El significado de la columna cambia con el paso del tiempo o la metodología para medir esa variable.

- Mucha / muy poca resolución

- Los valores `missing` no son realmente faltantes (`NAs`), si no que significan algo
    - Regularmente no documentado
    
- Si es un `csv` de seguro a alguien ya le pareció chistoso ponerle comas 
    - Existe una historia parecida para los `tsv`, `psv`, etc.

### `summary`

Un uso del  `summary()` es detectar problemas en los datos.

- ¿Valores faltantes?
    - ¿Hay una variable con muchos faltantes? ¿Por qué? ¿Es un error? ¿Significa algo?
    
- ¿Valores inválidos?
    - ¿Hay negativos donde no debería de haber? (Como en edad, ingreso, estatura)
    - ¿Texto en lugar de números?


- ¿Outliers?
    - Son aquellos valores que no crees que deberían de estar (En el ejemplo de edad 1400 años)

- ¿Rangos?
    - Es importante saber cuanto varía la variable.
    - Si es muy amplio, puede ser un problema para algunos algoritmos de modelado.
    - Si varía muy poco (o nada) no puede ser usado como predictor.
    
- ¿Unidades?
    - ¿El salario es mensual?¿Quincenal?¿Por hora?
    - ¿Los intervalos de tiempo están en segundos?¿Años?
    - ¿Las longitudes? ¿La moneda?


**Ejercicio**

Revisa `german_data` con `summary()`, reporta alguna anomalía.


```{r}
summary(german_data)
```
**Ejercicio**

Asegurar que el *dataset* esté en forma *tidy*. Guarda esto en `german-tidy.rds`


## Algas

### ¿Quién eres? 

Como el dinero no alcanza, tomas otro trabajo rápido para una ONG. Quieren predecir 
la concentración de algas en ríos de la región. Tomaron datos durante un año. 

Cada observación es el efecto de agregar varias muestras de agua recolectadas 
en el mismo río por un periodo de 3 meses en la misma estación del año.


### Datos 

Los datos provienen de [Coil 1999 Competition 
Data](https://archive.ics.uci.edu/ml/datasets/Coil+1999+Competition+Data) sobre contaminación de ríos.
La explicación de los datos se puede ver [aquí](https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/coil.data.html)

```{r}
algas_url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/coil-mld/analysis.data'

algas <- read_csv(algas_url, 
                  col_names = algas_colnames,
                  na = 'XXXXXXX')
```


** Ejercicio **

- Repite los pasos realizados para `german.data` con `algas`

- No te olvides de remover los `_` en las variables `river_size` y `fluid_velocity`

- Revisa con `summary()`, reporta alguna anomalía.



```{r}
summary(algas)
```

¿Por qué la columna `NO3` **no** es numérica?

```{r}
problems(algas)
```

El problema lo podemos observar, por ejemplo, en la columna `20`

```{r}
algas[20,]
```

Otra cosa interesante a notar, es que hay justo `r length(problems(algas))` casos de `a7` con `NA`s. 
Al parecer el error en la columna de `NO3` se está "comiendo" a la columna `a7`.

```{r}
algas[problems(algas)$row,]
```

La columna de `NO3` está capturada a 5 decimales. Parece lógica la suposición de 
dividir esa columna a los 5 decimales.

Podemos limpiar esta columna haciendo lo siguiente

```{r}
problematic_rows <- problems(algas)$row

algas[problematic_rows,] <- algas %>% 
    slice(problematic_rows) %>% 
    unite(col="all", -seq(1:6), sep = "/", remove=TRUE) %>%
    extract(all, into=c("NO3", "NH4", "resto"), regex="([0-9]*.[0-9]{5})([0-9]*.[0-9]*)/(.*)/NA", remove=TRUE) %>%
    separate(resto, into=names(algas)[9:18], sep="/", remove=TRUE)    

algas
```


```{r}
algas <- readr::type_convert(algas)

algas
```


```{r, warning=FALSE}
algas <- algas %>%
            mutate_each(funs(algas_clean))

algas
```


Revisando el resumen estadístico con  `summary`:

```{r}
summary(algas)
```

Los tipos de datos:

```{r}
glimpse(algas)
```


### Usando gráficas

- El resumen estadístico quizá no cuente toda la historia.

- El siguiente paso es explorar mediante gráficas.

- Es un proceso iterativo.

#### Una sola variable 

- ¿Cuál es el pico? ¿Coincide con la media? ¿La mediana? ¿Existe?
- ¿Cuántos picos?
  - Si es `bimodal` o `multimodal` quizá haya varias poblaciones en lugar de una y será mejor modelar por separado.
- ¿Qué tan normal es? ¿El log-normal? 
    - Utiliza una gráfica `Q-Q`
- ¿Cuánto varía? ¿Está concentrada en un intervalo o una categoría?
- ¿Outliers? -> Usa gráficas de `boxplot`.
- Da preferencia a los `density plots`, en esta gráfica es más importante la forma que los valores actuales del eje vertical.
- Si los datos están concentrados en un solo lado de la gráfica (`skewed`) y es no negativa es bueno representarla en `log10`.
- Una grafica de barras no da más información que  `summary()`, aunque alguna gente las prefiere.
    - Es bueno mostrarla horizontal y ordenada.

#### Dos variables
- ¿Existe relación entre dos variables de entrada? ¿entre una entrada y la variable de salida?
- ¿Qué tan fuerte?
- ¿Qué tipo de relación?
- `Scatter plot` entre dos variables numéricas, calcular la correlación de Pearson en un conjunto `sano` de los datos, visualizar la curva que mejor representa los datos.
- `Stacked bar charts` para dos variables categóricas.
    - Si quieres comparar razones a lo largo de las categorías lo mejor es usar un `filled bar chart`. En este caso se recomienda agregar un  `rug` para tener una idea de la cantidad de individuos.
    - Si hay múltiples categorías por variable, es mejor usar `facets`.
- Para variable categórica y numérica es recomendable usar `boxplot` (en su versión de `violin` o `jitter`).

**Ejercicio**

- Es importante en la etapa de exploración, poder generar varias gráficas de manera automática 
  y simple para analizarlas visualmente y tener una idea de los datos. 
- Crea una función que genere los tipos de gráfica para cada par de variables del `data.frame` (en realidad es un `tibble`).
  Esta función debe de recibir dos parámetros, uno que indique si genera todas las combinaciones 
  de dos variables o recibe una lista de variables en las cuales generar las combinaciones.
- Guárdala en `utils.R`. 
- Crea en `03-eda.R` en ambas carpetas: `algas` y `german`.


### Valores faltantes: `NAs`

- Los pasos son los siguientes:
    - Identificar los datos faltantes.
    - Examinar las causas de los datos faltantes. 
        - Preguntar al domain expert, etc.
    - Borrar los casos (o columnas) que contienen los `NAs` o reemplazar (imputar) los `NAs` con valores razonables.
    
- La teoría la veremos más adelante en el curso.

**NOTA**: Todas estás recomendaciones aplican igual para *outliers*

- Es importante recordar que en `R` la operación `x == NA` nunca regresa `TRUE`, siempre hay que utilizar las funciones `is.na()`, `is.nan()` e `is.infinite()`.

- El método `complete.cases` identifica los renglones (individuos) del `data.frame` que no tienen ningún `NA` en sus columnas (variables).

- Es posible usar `sum` y `mean` con `is.na` para obtener el total por columna de faltantes y el porcentaje.
    - ¿Por qué?

Aunque más adelante veremos técnicas más poderosas, vale la pena mencionar 
el ejemplo mostrado en `R in action`, cap. 15. 

La técnica nos permite determinar si los faltantes en una variable están correlacionados con otra.
```{r, eval=FALSE}
x <- as.data.frame(abs(is.na(df))) # df es un data.frame

head(df)

head(x)

# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)] 

# Da la correación un valor alto positivo significa que desaparecen juntas.
cor(y) 
```


**Ejercicio**

- Genera un reporte para ambos conjuntos de datos el estado de los valores missing.

- Muestra la matriz de correlación faltante en una gráfica.

- ¿Qué puedes entender?

### Variables faltantes: Remover observaciones

Las variables con más faltantes son Promedio de Cloruro `Cl` (10) y Promedio de Clorofila `Chla` (12).

```{r}
summary(algas[-grep(colnames(algas),pattern = "^a[1-9]")]) # Nota el uso del grep
```

También se puede hacer con `dplyr`

```{r}
algas %>%
    select(-starts_with("a")) %>%
    summary()
```



### Variables faltantes: Remover observaciones 

Antes de removerlas es recomendable verlos, guardarlos y contarlos:

```{r, eval=FALSE}
nrow(algas[!complete.cases(algas),])
```
Hay `r nrow(algas[!complete.cases(algas),])` observaciones en las cuales tienen `NAs`

```{r}
algas_con_NAs <- algas[!complete.cases(algas),]
```

Siempre es bueno guardarlas, si se piensan eliminar del dataset.

Las observaciones con `NAs`son las siguientes (usaremos la función `print()` para explorar)

```{r}
algas_con_NAs[c('max_PH', 'min_O2', 'Cl', 'NO3', 'NH4', 'oPO4', 'PO4', 'Chla')]  %>%
    print(n = 33)
```

de los cuales, hay dos renglones que tienen más del `50%` (`6`) de 
las variables independientes nulas.


Aunque remover las observaciones con `NAs` **NO** sea la estrategia, quitar las observaciones 
con muchas columnas vacías, puede ser recomendable.

En los casos en los que no es posible hacer una explorarción visual, se puede utilizar el siguiente código

```{r}
# ¿Cuántos NAs hay por observación?
apply(algas, 1, function(x) sum(is.na(x)))
```
Si queremos ver las observaciones:
```{r}
algas[apply(algas, 1, function(x) sum(is.na(x))) > 2,]
```
Lo cual confirma nuestra exploración visual.

Si eliminar las observaciones con `NAs` va a ser el camino que vamos a tomar, habrá que hacerlo de manera
reproducible, lo que sigue es el código de la función `indices_con_NAs'

```{r}
indices_con_NAs
```

```{r}
indices_con_NAs(algas, 0.2)
```

```{r}
indices_con_NAs(algas, 0.8)
```

```{r, eval=FALSE}
# Si queremos remover las que tengan más del 20% de NAs...
algas <- algas[-indices_con_NAs(algas, 0.2),]
```

### Variables faltantes: Renivelar 

- Si la variable es categórica (`factor`), puedes crear una nueva variable y poner los `NA`s a un nuevo `level`, e.g. `missing`

    - Por ejemplo, suponiendo que hubiese una variable categórica con faltantes en nuestros dataset
  
```{r eval=FALSE}
dataset$cat_with_NAs_fix <- ifelse(is.na(dataset$cat_with_NAs),
                              "missing",
                              ifelse(dataset$ccat_with_NAs == TRUE,    
                                                # o el valor que sea
                                                "level_1",
                                                "level_2"))
```   

- Sólo recuerda que es posible que el valor de `NA` signifique algo.

- Esto también se puede hacer con variables numéricas, si primero las vuelves categóricas (i.e. *binning*)

### Variables faltantes: Central

- Una estrategia es rellenar los valores faltantes con alguna medida de centralidad.
    - Media, mediana, moda, etc.

- Para variables distribuidas normalmente, esta opción es la mejor.

- Pero para variables *skewed*  o con *outliers* esta decisión puede ser desastrosa.

- Por lo tanto, esta estrategia no se debe de utilizar salvo una exploración previa de las variables.

#### Ejemplo

- ¿A qué variables le puedes de `algas` le puedes aplicar este procedimiento?

- ¿Qué puedes decir de `german_data`?

- A las variables que no se les puede aplicar, explica por qué no.

- Esta decisión debe de ser reproducible, agrega a `utils.R` una función que impute 
en las variables con  `NAs` el valor central (`median` si es numérica, `moda` si es categórica).
La función debe de tener la siguiente firma:

```{r, eval=FALSE}
imputar_valor_central <- function(data, colnames) {...}
```

### Variables faltantes: Correlación

Calculando rápidamente la correlación

```{r}
algas %>%
    select(-c(1:3)) %>%
    cor(use="complete.obs") %>%
    symnum()
```

Observamos que  `oPO4` y `PO4` están altamente relacionadas (`> 0.9`).


```{r correlacion, warning=FALSE, fig.height=4, fig.width=8}
ggplot(data=algas) + 
  aes(x=oPO4, y=PO4) + 
  geom_point(shape=1) + # Usamos una bolita para los puntos
  geom_smooth(method=lm, se=FALSE) +
    theme_hc()
  # Mostramos la linea de la regresión y no mostramos la región de confianza
```
```{r}
algas
```

```{r}
#algas <- algas[-indices_con_NAs(algas, 0.2),]
modelo <- lm(PO4 ~ oPO4, data=algas)
modelo
```

Entonces la fórmula que relaciona el `PO4` con `oPO4` es

$$
PO4 = `r modelo$coefficients['(Intercept)']` + `r modelo$coefficients['oPO4']`*oPO4
$$

**Ejercicio**
Crea una función que sustituya los `NAs` con el valor dado por la 
regresión lineal recién calculada (No automatices la regresión lineal) usando la
siguiente firma

```{r, eval=FALSE}
imputar_valor_lm <- function(var_independiente, modelo) { ... }
```

### Variables faltantes: Similitud

- Podemos suponer que si dos observaciones son similares y una de ellas tiene `NAs` 
en alguna variable, hay una alta probabilidad de que esa variable tenga un valor
similar al valor de esa variable en la otra observación.
    - Obviamente es una suposición...
    
- Debemos definir la noción de similar
    - Y esto significa definir un espacio métrico en el espacio que
       usamos para describir las observaciones.
    - Obviamente, otra gran suposición...

- Para variables numéricas se puede usar la distancia euclídea

$$
d(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^p(\vec{x}_i - \vec{y}_i)}
$$

- Si son nominales las variables

$$
d(\vec{x}, \vec{y}) = \sqrt{\sum_{i=1}^p \delta_i(\vec{x}_i, \vec{y}_i)}
$$

donde $\delta(\vec{x},  \vec{y})$ es a delta de Kronecker.


- Una vez definida la similitud, debemos de definir el valor que imputar al `NA`.
    
- Una opción es utilizar una medida de centralidad de los $k$ observaciones más cercanas. 

- El Promedio con peso de los valores de los vecinos, es otra opción. El peso se puede determinar de varias
maneras, pero usar como *kernel* una función gaussiana.

$$
peso(d) = e^{-d}
$$

donde $d$ es la distancia de una observación a la que estamos considerando.


- Es importante normalizar los valores numéricos antes de calcular las distancias.

$$
\vec{x}_{normalizado} = \frac{\vec{x}_i - \bar{x}}{\sigma_{x}}
$$

**Ejercicio:** ¿Por qué?


**Ejercicio** 

- Implementen una función que impute por similitud con la firma

```{r, eval=FALSE}
imputar_por_similitud <- function(data, num_vecinos) { ... }
```

- Aplícalo a `algas` y `german`. 

- ¿Son muy diferentes las estadísticas ignorando los `NAs` comparadas con este método?

## Transformación de datos 

- Normalizar
    - Es útil cuando las cantidades absolutas son menos importantes que las relativas.
    
- Normalizar y reescalar 
    - Usar la desviación estándar como unidad de medida.
    - Tiene mucho sentido si la distribución es simétrica.
    - Si no lo es, es posible que sea *lognormally distributed* 
     (como el ingreso monetario o los gastos), una transformación `log10()`
     lo hará útil.

## Transformación de los datos 

- Es una buena idea usar `log` si el rango de tus datos cubre varios ordenes de magnitud. 
    - Regularmente, estas variables vienen de procesos **multiplicativos** en lugar de aditivos. 

- Si el rango incluye cantidades negativas, usa (crea) una función `signedLog10`

```{r eval=FALSE}
signedLog10 <- function(x) {
  ifelse(abs(x) <= 1.0, sign(x)*log10(abs(x)))
}
```

**Ejercicio**


- En este momento es quizá una buena idea, dejar de duplicar código y concentrar 
  todas las funciones de `utils.R` que se puedan reutilizar en un archivo `toolset.R`.
  Ajusta tus demás archivos de acordemente.


- Crea un `R notebook` para cada *dataset* utilizando los archivos reproducibles y el archivo
  `toolset.R`. Incluye en estos `notebook` la estructura de los datos, *GEDA* transformaciones de los datos 
  y observaciones pertinentes (como *outliers*, estructura de los datos faltantes). 
  Explica los métodos de imputación que usaste (si fué necesario) y porqué los usaste.


## EDA  $\to$ modelado

- En general la exploración de datos se divide en tres pasos:
    - Verificar la distribución de las variables individuales
        - Identificando *outliers*, valores faltantes $\to$ transformación, eliminación del *dataset*, etc.
    - Verificar la relación entre las variables dependientes y los predictores 
        - Se podrá usar en `feature selection`
    - Relación entre los predictores
        - Eliminación de variables redundantes

# Datos relacionales y la cuestión de la Semántica

## Titanic

![Titanic](../data/Titanic/images/titanic.jpg)



```{r}

titanic_path <- '../data/Titanic/titanic.ods'

ds_names <- ods_sheets(titanic_path)
ds_names
```

Arreglemos los nombres de los *data sets*

```{r}
clean_sheet_name <- function(sheet_name) {
    str_to_lower(str_replace_all(str_replace_all(string=sheet_name, pattern=" ", replace="_"), pattern="'", replace=""))
}

sapply(X = ds_names, FUN = clean_sheet_name)
```

Ahora obtenemos los *datasets* y los guardamos

```{r warning=FALSE,message=FALSE,include=FALSE}
save_sheet <- function(sheet_name) {
    file_name <-  paste("../data/Titanic/", clean_sheet_name(sheet_name), ".rds", sep = "")
    saveRDS(object = read_ods(titanic_path, sheet = sheet_name), file = file_name)
}


lapply(ods_sheets(titanic_path), save_sheet)
```

**Ejercicio**

- En algunos *data sets* se agregaron columnas de más, remuévelas
- Genera las siguientes variables: `survived`, `name`, `last_name`, `sex`
- Agrega una columna de `age` que sea categórica
- Arregla la columna de precio, edad
- Ajusta a precios del día de hoy (Por ejemplo usa esta [página](http://inflation.stephenmorley.org/))¿En que clase hubieras viajado? ¿Cuál era tu probilidad de supervivencia?
- Crea un método para unificar los *data sets* en uno solo.
- Observando la distribución de botes que se muestra en la figura ¿Qué puedes decir sobre como se utilizaron?
  ¿Coincide con la película de Titanic de James Cameron?

![botes](../data/Titanic/images/deckplan_boat.gif)


## Berka

Los datos están en `http://sorry.vse.cz/~berka/challenge/pkdd1999/data_berka.zip`, descárgalos y 
guárdalos en `data/berka`, fueron usados en la competencia de PKDD de 1999. 

### Meta

"El banco busca mejorar sus servicios, Por ejemplo, los admnistradores del banco 
tienen únicamente una vaga idea de quién es un buen cliente (al cual ofrecerle
más servicios) y quién es un mal cliente (alguien al que hay que cuidar para 
minimizar las pérdidas del banco). Afortunadamente, el banco almacena datos 
de sus clientes, sus cuentas (transacciones), préstamos que ya se otorgaron, 
tarjetas de crédito dadas. Los administradaores del banco esperan mejorar
su entendimiento de los clientes y mejorar así sus servicios. Una mera aplicación
de una herramienta de *discovery* no los va a convencer."


### Esquema

#### Account

Características de la cuenta
Archivo: `account`

 Column|Description|Notes
--------|------------|---------
account_id	|Identification of the account|
district_id	|Location of the branch	 |
date	|Date of the account's creation	|In the form: YYMMDD 
frequency	|Frequency of statement issuance|	"POPLATEK MESICNE" - Monthly Issuance
| | |"POPLATEK TYDNE" - Weekly Issuance
| | | "POPLATEK PO OBRATU" - Issuance After Transaction


#### Client

Características del cliente
Archivo: `client`

Column	|Description|	Notes
--------|------------|---------
client_id	|Client Identifier	 |
birth number	|Birthday and Sex |	The value is in the form: YYMMDD (for men)
| | | The value is in the form: YYMM+50DD (for women)
| | |Where YYMMDD is the date of birth
district_id	|Address of the client	 |

	

#### Disposition 

Relaciona una cuenta con un cliente (los derechos  de los clientes para
operar cuentas)
Archivo: `disp`

Column	|Description	|Notes
--------|------------|---------
disp_id	|Record Identifier	 |
client_id	|Client Identifier|	 
account_id	|Account Identifier	 |
type	| Type of Disposition (owner/user)	|Only owner can issue permanent orders and ask for a loan

#### Loan

Préstamos dabos a una cuenta
Archivo: `loan`

Column	|Description	|Notes
--------|------------|---------
disp_id	|Record Identifier	 |
loan_id	|Record Identifier	 |
account_id	|Account Identifier	 |
date	|Date when loan was granted	|In the form: YYMMDD
amount	|Amount of Loan	 |
duration	|Duration of Loan	 |
payments	|Monthly Payments on Loan	 |
status	|Status in paying off the loan	|'A' stands for contract finished, no problems
| | | 'B' stands for contract finished, loan not payed
| | | 'C' stands for running contract, OK thus-far
| | | 'D' stands for running contract, client in debt

#### Order

Características de una orden de pago
Archivo: `order`

Column	|Description	|Notes
--------|------------|---------
order_id	|Record Identifier	 |
account_id	|Account the order is issued for	 |
bank_to	|Bank of the recipient	|Each bank has a unique two-letter code
account_to|	Account of the recipient	 |
amount	|Amount debited from order account |	 
K_symbol|	Characterization of the payment|	'POJISTNE' stands for Insurance Payment
| | |'SIPO' stands for Household Payment
| | |'LEASING' stands for Leasing Payment
| | | 'UVER' stands for Loan Payment

#### Transaction

Transacciones en las cuentas
Archivo: `trans`

Column	|Description	|Notes
--------|------------|---------
trans_id |	Record Identifier	 |
account_id	|Account the transaction is issued on	 |
date	|Date of transaction	|In the form: YYMMDD
type	|debit/credit transaction	|'PRIJEM' stands for Credit
| | | 'VYDAJ' stands for Debit (withdrawal)
operation	|Mode of Transaction	|'VYBER KARTOU' stands for Credit Card Withdrawal
| | |'VKLAD' stands for Credit in Cash
| | |'PREVOD Z UCTU' stands for Collection from Another Bank
| | |'VYBER' stands for Withdrawal in Cash
| | | 'PREVOD NA UCET' stands for Remittance to Another Bank
amount	|Amount of Transaction	 |
balance	|Balance of Account after Transaction	 |
K_Symbol	|Characterization of Transaction	|'POJISTNE' stands for Insurance Payment
| | |'SLUZBY' stands for Payment of Statement
| | |'UROK' stands for Interest Credited
| | |'SANKC. UROK' stands for Sanction Interest if Negative Balance
| | |'SIPO' stands for Household Payment
| | |'DUCHOD' stands for Old-age Pension Payment
| | |'UVER' stands for Loan Payment
bank	|Bank of the partner	|Each bank has unique two-letter code
account	|Account of the partner	 |

#### Demographic

Características  de un distrito
Archivo: `district`


Column	|Description	|Notes
--------|------------|---------
A1 | district_id|	District Identifier	 
A2	|District Name	 
A3	|Region	 
A4	|No. of Inhabitants	 
A5	|No. of Municipalities with inhabitants < 499	 
A6	|No. of Municipalities with inhabitants 500-1999	 
A7	|No. of Municipalities with inhabitants 2000-9999	 
A8	|No. of Municipalities with inhabitants > 10000	 
A9	|No. of Cities	 
A10	|Ratio of urban inhabitants	 
A11	|Average Salary	 
A12	|Unemployment rate in 1995	 
A13	|Unemployment rate in 1996	 
A14	|No. of Enterpreneurs per 1000 inhabitants	 
A15	|No. of Crimes commited in 1995	 
A16	|No. of Crimes commited in 1996

#### Credit card

Tarjeta de crédito otorgada a una cuenta
Archivo: `card` 

Column	|Description	|Notes
--------|------------|---------
card_id	|Card Identifier	 |
disp_id	|Disposition to an account	 |
type	|Type of card	|Types are 'Junior', 'Classic', and 'Gold'
issued	|Date card was issued	|In the format: YYMMDD

### Características importantes

- Cada cuenta tiene características estáticas (`account`) y dinámicas
(pagos, créditos, balances) dados en `order` y `transaction`
- `client` características de personas que pueden manipular cuentas. 
- Los clientes pueden tener varias cuentas, y viceversa. La relación entre clientes y cuentas está en `disposition`
- Los servicios del banco están descritos en `loan` y `credit_card`
- Una cuenta puede tener varias **tdc**
- Máximo un préstamo se le puede otorgar a una cuenta.
- La tabla de demografía contiene informacióń sobre los distritos, se puede
deducir información adicional sobre los clientes a partir de esta tabla.

#### Esquema Entidad-Relación

![Berka dataset](http://lisp.vse.cz/pkdd99/Challenge/data.gif)

### Base de datos

La utilización de una base de datos (Relacional, Grafos, Columnar, etc), siempre es recomendable sobre 
el uso de archivos (la excepción es un Apache Hadoop, pero eso es harina de otro costal).

Las ventajas de la utilización de una base de datos son las siguientes:

- Es posible manejar volúmenes de datos más grandes que memoria.
- Velocidad de acceso a los datos
- Facilidad de manipulación con lenguajes relacionales (como `SQL`)
- Colaboración
    - Varias personas pueden accesar a los datos
    - Estas personas no tienen por qué usar el mismo lenguaje (!)

Usaremos `sqlite3`  para este ejemplo. `SQLite` es una base de datos relacional, local
(por lo que algunas de las ventajas recién mencionadas no aplican ¿Cuál?)

*NOTA: Lo que sigue ocurre en la línea de comandos*

Para abrir una base de datos `sqlite` en memoria

```{shell eval=FALSE}
$ sqlite3
SQLite version 3.11.0 2016-02-15 17:29:24
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
sqlite> 
```

```{shell eval=FALSE}
## Emebelliciendo el output
sqlite> .header on
sqlite> .mode column


## Importar archivos (accounts.asc) a la base de datos a la tabla (accounts)
## ¡Usa plural para las tablas!
sqlite> .import account.asc accounts

## Verifica que la tabla exists
sqlite> .tables
acccounts

## Estructura de la tabla
sqrlite> PRAGMA tabl<e_info(accounts)
cid         name        type        notnull     dflt_value  pk        
----------  ----------  ----------  ----------  ----------  ----------
0           account_id  TEXT        0                       0         
1           district_id TEXT        0                       0         
2           frequency   TEXT        0                       0         
3           date        TEXT        0                       0    

## Guardar la tabla
sqlite> .save berka.raw
```

La siguiente vez que quieras consultar la base de datos

```{shell eval=FALSE}
$ sqlite3 berka.raw
SQLite version 3.11.0 2016-02-15 17:29:24
Enter ".help" for usage hints.
sqlite> 
```

O si se te olvida poner el nombre del archivo:

```{shell eval=FALSE}
$ sqlite3
SQLite version 3.11.0 2016-02-15 17:29:24
Enter ".help" for usage hints.
Connected to a transient in-memory database.
Use ".open FILENAME" to reopen on a persistent database.
sqlite> .open berka.raw
```



**Ejercicio: To Raw**

Cargaremos los datos en `berka.raw`

**Ejercicio: Raw to Clean**

- Verifica que cada `account` tenga un `owner`
- Los registros de  `orders` y `loans` están duplicados en `transactions`. Es decir
los regsitros de `order`y `loan` están dentro de  `transactions` (por ejemplo, 
Los registros de `loan` en `tran` están identificados por el `k_symbol` `LP`)
- Traduce los campos del checo al inglés.
- En `client` cambia `BirthNumber` a `sex` y `age`
- Discretiza `age` en `Youth` (0-24), `Adult` (25-45), `Middle-age` (46-64) y `Senior` (> 65)
- En `disposition` cambia de `Dispondent` -> `User` 
- En `loan` discretiza usando alguna heuristica `amount`, `duration`  y `payments`
- En `transaction` traduce la columna `type`, `operation`, `k_symbol`
- Ajustamos los nombres de las tablas a plural, los `*_id` a singular.
- Guardemos los datos en `berka.clean`

**Ejercicio: Clean to Semantic**

- Denormalicemos los datos
- ¿Cuál es el objeto que nos interesa? ¿Tarjeta? ¿Transacción? ¿Cliente?
- ¿Cómo quedarían las tablas?
- Guardemos los datos en `berka.db`

Podemos accesar a la base de datos usando `dplyr`

```{r}
berka_db <- dplyr::src_sqlite(path="../data/berka/berka.db")
#accounts <- dplyr::tbl(berka_db, "accounts")
```


# Limpieza: Retos técnicos


![Automatic Schema Mapping](../images/tamr_a.png)
Fuente: *Ihab Ilyas* de **Tamr**



![](../images/tamr_b.png)
Fuente: *Ihab Ilyas* de **Tamr**


![](../images/tamr_c.png)
Fuente: *Ihab Ilyas* de **Tamr**


![](../images/tamr_d.png)
Fuente: *Ihab Ilyas* de **Tamr**


![](../images/tamr_e.png)
Fuente: *Ihab Ilyas* de **Tamr**


![](../images/tamr_f.png)
Fuente: *Ihab Ilyas* de **Tamr**


# Primer acercamiento a Linage and Procedence: Columnas de procedencia  

- Un aspecto que siempre es olvidado, o que no se considera importante debido a los `blogs`,
  es el **versionado de control de los datos**

- Esto se puede implementar, agregando columnas para indicar de donde vienen los datos, 
  o con qué procedimiento de limpieza se generaron, etc.
    - Se puede utilizar el mismo `id` del código del ETL guardado en `github`.
- ¿Cómo implementarías esto? ¿Puedes trazar de dónde provienen tus datos?
